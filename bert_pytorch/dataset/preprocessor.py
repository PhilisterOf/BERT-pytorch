# src/lad_bert/data/preprocessor.py
import re
import os
import pandas as pd
from datetime import datetime
from tqdm import tqdm


class ParserFreePreprocessor:
    def __init__(self, dataset_type='hdfs', window_size=20, step_size=5):
        self.dataset_type = dataset_type.lower()
        self.window_size = window_size
        self.step_size = step_size

    def clean_log_content(self, content):
        # ... (ä¿æŒä¹‹å‰çš„æ¸…æ´—é€»è¾‘ä¸å˜) ...
        content = content.lower()
        content = re.sub(r'(\d{1,3}\.){3}\d{1,3}', '[IP]', content)
        if 'blk_' in content:
            content = re.sub(r'blk_[-0-9]+', '[BLK]', content)
        content = re.sub(r'\b\d+\b', '[NUM]', content)
        content = re.sub(r'([^\w\s\[\]])', r' \1 ', content)
        content = re.sub(r'\s+', ' ', content).strip()
        return content

    def process_hdfs(self, log_file):
        print(f"æ­£åœ¨å¤„ç† HDFS (å¸¦æ—¶é—´): {log_file} ...")

        # 1. å®šä¹‰æ­£åˆ™ï¼šåŒæ—¶æå–æ—¥æœŸã€æ—¶é—´ã€BlockIDã€Content
        # æ ·æœ¬: 081109 203518 143 INFO ...
        # Group 1: Date (081109)
        # Group 2: Time (203518)
        # Group 3: Block ID
        # Group 4: Content (å‰©ä¸‹çš„)
        pattern = re.compile(r'^(\d{6})\s+(\d{6}).*?(blk_[-0-9]+)(.*)')

        grouped_data = {}  # {blk_id: [{'timestamp': float, 'content': str}, ...]}

        with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
            for line in tqdm(f):
                match = pattern.search(line)
                if match:
                    date_str, time_str, blk_id, raw_content = match.groups()

                    # 2. è§£ææ—¶é—´æˆ³
                    # HDFS æ ¼å¼: yymmdd HHMMSS
                    dt = datetime.strptime(f"{date_str}{time_str}", "%y%m%d%H%M%S")
                    timestamp = dt.timestamp()

                    # 3. æ¸…æ´—æ–‡æœ¬
                    cleaned_content = self.clean_log_content(raw_content)

                    if blk_id not in grouped_data:
                        grouped_data[blk_id] = []

                    grouped_data[blk_id].append({
                        'timestamp': timestamp,
                        'content': cleaned_content
                    })

        print("åˆ†ç»„å®Œæˆï¼Œå¼€å§‹è®¡ç®—æ—¶é—´é—´éš”...")

        final_sequences_text = []
        final_sequences_time = []

        # 4. å¯¹æ¯ä¸ª Block å†…éƒ¨æ’åºå¹¶è®¡ç®— Delta T
        for blk_id, logs in grouped_data.items():
            # æŒ‰æ—¶é—´æ’åº
            logs.sort(key=lambda x: x['timestamp'])

            # æå–æ–‡æœ¬åˆ—è¡¨
            text_seq = [x['content'] for x in logs]

            # è®¡ç®—æ—¶é—´é—´éš” (å½“å‰æ—¶é—´ - ä¸Šä¸€æ¡æ—¥å¿—æ—¶é—´)
            # ç¬¬ä¸€æ¡æ—¥å¿—é—´éš”ä¸º 0
            time_seq = [0.0]
            for i in range(1, len(logs)):
                delta = logs[i]['timestamp'] - logs[i - 1]['timestamp']
                # ä¸ºäº†é˜²æ­¢æ—¶é—´è¿‡å¤§å½±å“æ¨¡å‹ï¼Œé€šå¸¸å– log æˆ–è€…æˆªæ–­ï¼Œè¿™é‡Œå…ˆä¿ç•™åŸå§‹å€¼
                time_seq.append(float(f"{delta:.4f}"))

            final_sequences_text.append(text_seq)
            final_sequences_time.append(time_seq)

        return final_sequences_text, final_sequences_time

    def process_bgl(self, log_file):
        """
        BGL ä¸“ç”¨å¤„ç† (ä¹Ÿé€‚ç”¨äº Thunderbirdï¼Œæ ¼å¼éå¸¸åƒ)
        BGL æ ¼å¼: Label Timestamp Date Node Time NodeRepeated Type Content...
        ä¾‹å­: - 1117838570 2005.06.03 R02-M1-N0-C:J12-U11 ...
        """
        print(f"æ­£åœ¨å¤„ç† {self.dataset_type.upper()} (æ»‘åŠ¨çª—å£ + æ—¶é—´): {log_file} ...")

        all_logs = []  # å­˜å‚¨æ‰€æœ‰è§£æå¥½çš„ {'timestamp': t, 'content': c}

        # BGL çš„æ—¶é—´æˆ³é€šå¸¸åœ¨ç¬¬äºŒåˆ— (index 1)ï¼Œæ˜¯ Unix Timestamp
        # å¦‚æœä½ çš„ BGL æ•°æ®æ ¼å¼ä¸åŒï¼Œè¿™é‡Œå¯èƒ½è¦å¾®è°ƒ

        with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
            for line in tqdm(f, desc="è¯»å–åŸå§‹æ—¥å¿—"):
                parts = line.strip().split()
                if len(parts) < 5:
                    continue

                # 1. æå– Label (å¯é€‰ï¼Œå¦‚æœæ˜¯è®­ç»ƒé›†ï¼Œé€šå¸¸åªä¿ç•™ Label ä¸º '-' çš„æ­£å¸¸æ—¥å¿—)
                label = parts[0]
                # if label != '-': continue # å¦‚æœåªæƒ³è®­ç»ƒæ­£å¸¸æ—¥å¿—ï¼ŒæŠŠè¿™è¡Œæ³¨é‡Šæ‰“å¼€

                # 2. æå–æ—¶é—´æˆ³ (BGL çš„ç¬¬2åˆ—é€šå¸¸æ˜¯ Unix æ—¶é—´æˆ³)
                try:
                    timestamp = float(parts[1])
                except ValueError:
                    continue  # æ ¼å¼é”™è¯¯çš„è¡Œè·³è¿‡

                # 3. æå– Content
                # BGL çš„æ­£æ–‡é€šå¸¸ä»ç¬¬ 9 åˆ—æˆ–è€…ç¬¬ 10 åˆ—å¼€å§‹ï¼Œå‰é¢éƒ½æ˜¯å…ƒæ•°æ®
                # ä¸ºäº†å·æ‡’ä¸”ä¿è¯ä¿¡æ¯å…¨ï¼Œæˆ‘ä»¬å¯ä»¥å–ç¬¬ 5 åˆ—å¾€åçš„æ‰€æœ‰å†…å®¹
                raw_content = " ".join(parts[4:])

                # 4. æ¸…æ´—
                cleaned_content = self.clean_log_content(raw_content)

                all_logs.append({
                    'timestamp': timestamp,
                    'content': cleaned_content
                })

        # æŒ‰ç…§æ—¶é—´æ’åº (é˜²æ­¢åŸå§‹æ—¥å¿—ä¹±åº)
        all_logs.sort(key=lambda x: x['timestamp'])

        print(f"å…±è¯»å– {len(all_logs)} æ¡æ—¥å¿—ï¼Œå¼€å§‹æ‰§è¡Œæ»‘åŠ¨çª—å£åˆ‡åˆ†...")
        print(f"çª—å£å¤§å°: {self.window_size}, æ­¥é•¿: {self.step_size}")

        final_texts = []
        final_times = []

        # === æ ¸å¿ƒç®—æ³•ï¼šæ»‘åŠ¨çª—å£ç”Ÿæˆ Time Embedding ===
        num_logs = len(all_logs)
        for i in tqdm(range(0, num_logs - self.window_size, self.step_size), desc="ç”Ÿæˆçª—å£"):
            # åˆ‡ç‰‡
            window = all_logs[i: i + self.window_size]

            # 1. æ–‡æœ¬åºåˆ—
            text_seq = [x['content'] for x in window]

            # 2. æ—¶é—´åºåˆ— (è®¡ç®— Delta T)
            # çª—å£å†…çš„ç¬¬ä¸€æ¡æ—¥å¿—ï¼ŒDelta T è®¾ä¸º 0
            time_seq = [0.0]
            for j in range(1, len(window)):
                delta = window[j]['timestamp'] - window[j - 1]['timestamp']
                # åŒæ ·çš„ï¼Œä¿ç•™æµ®ç‚¹æ•°
                time_seq.append(float(f"{delta:.4f}"))

            final_texts.append(text_seq)
            final_times.append(time_seq)

        return final_texts, final_times

    def process(self, input_path):
        if self.dataset_type == 'hdfs':
            return self.process_hdfs(input_path)
        elif self.dataset_type in ['bgl', 'thunderbird']:
            return self.process_bgl(input_path)
        else:
            raise ValueError(f"æœªçŸ¥æ•°æ®é›†: {self.dataset_type}")

    def process(self, input_path):
        if self.dataset_type == 'hdfs':
            return self.process_hdfs(input_path)
        else:
            # BGL/TB çš„æ—¶é—´å¤„ç†é€»è¾‘ç±»ä¼¼ï¼Œéœ€è§£æå®ƒä»¬ç‰¹å®šçš„æ—¶é—´æ ¼å¼
            raise NotImplementedError("BGL/TB æ—¶é—´è§£ææš‚æœªæ·»åŠ ï¼Œè¯·å…ˆè·‘ HDFS")

# è´´åœ¨ preprocessor.py æœ€ä¸‹é¢
if __name__ == "__main__":
    # === 1. è®¾ç½®è·¯å¾„ ===
    # è¿™é‡Œçš„ ../.. å–å†³äºä½ è¿è¡Œè„šæœ¬æ—¶æ‰€åœ¨çš„ç›®å½•ã€‚
    # å¦‚æœä½ åœ¨ src/lad_bert/data/ ä¸‹è¿è¡Œï¼Œè¿™æ ·å†™æ˜¯å¯¹çš„ã€‚
    TEST_INPUT = "../../data/raw/HDFS.log"
    TEST_OUTPUT_DIR = "../../data/processed"

    if not os.path.exists(TEST_INPUT):
        print(f"âŒ æµ‹è¯•å¤±è´¥ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {TEST_INPUT}")
        # å»ºè®®æ‰“å°ä¸€ä¸‹å½“å‰å·¥ä½œç›®å½•ï¼Œæ–¹ä¾¿æ’æŸ¥è·¯å¾„é—®é¢˜
        print(f"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
    else:
        # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
        os.makedirs(TEST_OUTPUT_DIR, exist_ok=True)

        # === 2. è¿è¡Œå¤„ç† ===
        p = ParserFreePreprocessor(dataset_type='hdfs')
        texts, times = p.process(TEST_INPUT)

        # é¢„è§ˆ
        print(f"âœ… å¤„ç†å®Œæˆï¼Œå†…å­˜ä¸­ç”Ÿæˆäº† {len(texts)} æ¡æ•°æ®")

        # === 3. ä¿å­˜æ–‡ä»¶ (è¿™æ˜¯ä½ ä¹‹å‰ç¼ºå¤±çš„éƒ¨åˆ†ï¼) ===
        # ä¿å­˜æ–‡æœ¬æ–‡ä»¶
        out_text_path = os.path.join(TEST_OUTPUT_DIR, "hdfs_corpus_text.txt")
        print(f"æ­£åœ¨ä¿å­˜æ–‡æœ¬åˆ°: {out_text_path}")
        with open(out_text_path, 'w', encoding='utf-8') as f:
            for seq in texts:
                # ç”¨ [SEP] æ‹¼æ¥
                f.write(" [SEP] ".join(seq) + "\n")

        # ä¿å­˜æ—¶é—´æ–‡ä»¶
        out_time_path = os.path.join(TEST_OUTPUT_DIR, "hdfs_corpus_time.txt")
        print(f"æ­£åœ¨ä¿å­˜æ—¶é—´åˆ°: {out_time_path}")
        with open(out_time_path, 'w', encoding='utf-8') as f:
            for seq in times:
                # ç”¨é€—å·æ‹¼æ¥æ•°å­—
                f.write(",".join(map(str, seq)) + "\n")

        print("ğŸ‰ å…¨éƒ¨ä¿å­˜å®Œæ¯•ï¼")

# # === æµ‹è¯•ä»£ç  ===
# if __name__ == "__main__":
#     # å‡è®¾ä½ ä¸‹è½½äº† BGL çš„å‰ 2000 è¡Œåšæµ‹è¯•
#     TEST_INPUT = "../../data/raw/BGL_2k.log"
#     TEST_OUTPUT_DIR = "../../data/processed"
#
#     # ä¸ºäº†æµ‹è¯• BGLï¼Œè®°å¾—å®ä¾‹åŒ–æ—¶æŒ‡æ˜ dataset_type
#     p = ParserFreePreprocessor(dataset_type='bgl', window_size=20, step_size=5)
#
#     # ... (åé¢çš„ä¿å­˜ä»£ç å’Œä¹‹å‰ä¸€æ ·) ...