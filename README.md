ä¸ºäº†è®©ä½ å¯¹æ•´ä¸ªå®éªŒæµç¨‹çš„æ•°æ®æµå‘ï¼ˆData Pipelineï¼‰ä¸€ç›®äº†ç„¶ï¼Œæˆ‘å°†è¿™äº›æ–‡ä»¶æŒ‰**ç”Ÿæˆé˜¶æ®µ**å’Œ**åŠŸèƒ½ç”¨é€”**è¿›è¡Œäº†åˆ†ç±»æ ‡æ³¨ã€‚

è¿™äº›æ–‡ä»¶æ„æˆäº†ä½ è®ºæ–‡å®éªŒçš„å®Œæ•´è¯æ®é“¾ã€‚

### 1. æ ¸å¿ƒäº§ç‰© (The "Brain")
è¿™æ˜¯è®­ç»ƒè„šæœ¬æœ€ç»ˆç”Ÿæˆçš„ã€ä»·å€¼æœ€é«˜çš„æ–‡ä»¶ã€‚

*   **`best_model.pth`**
    *   **æ¥æº**: `train.py` åœ¨ Early Stopping è¿‡ç¨‹ä¸­ä¿å­˜çš„ã€‚
    *   **å†…å®¹**: åŒ…å«ä¸¤ä¸ªå…³é”®å¯¹è±¡ï¼š
        1.  `model_state_dict`: TinyBERT çš„æ‰€æœ‰æƒé‡å‚æ•°ï¼ˆå·²å­¦ä¼š HDFS è¯­æ³•ï¼‰ã€‚
        2.  `center`: è®­ç»ƒå¥½çš„è¶…çƒä½“ä¸­å¿ƒå‘é‡ï¼ˆæ­£å¸¸æ—¥å¿—çš„â€œèšç±»ä¸­å¿ƒâ€ï¼‰ã€‚
    *   **ç”¨é€”**: **æ¨ç†æ ¸å¿ƒ**ã€‚`predict.py` å°†åŠ è½½å®ƒæ¥åˆ¤æ–­æ–°æ—¥å¿—ç¦»è¿™ä¸ªä¸­å¿ƒæœ‰å¤šè¿œã€‚

### 2. åŸºç¡€è®¾æ–½ (Infrastructure)
è¿™æ˜¯è®© BERT ç†è§£æ—¥å¿—â€œè¯­è¨€â€çš„å­—å…¸ã€‚

*   **`vocab.txt`**
    *   **æ¥æº**: `train_tokenizer.py` (åŸºäº `train.txt` è®­ç»ƒç”Ÿæˆ)ã€‚
    *   **å†…å®¹**: çº¦ 3000 è¡Œã€‚æ¯è¡Œä¸€ä¸ª Tokenï¼ˆå¦‚ `[BLK]`, `packet`, `##ponder`ï¼‰ã€‚
    *   **ç”¨é€”**: **ç¿»è¯‘å®˜**ã€‚å®ƒè´Ÿè´£æŠŠæ–‡æœ¬æ—¥å¿—è½¬æ¢æˆæ•°å­— ID åºåˆ—ã€‚å®ƒæ˜¯ä½ â€œé¢†åŸŸè‡ªé€‚åº”ï¼ˆDomain Adaptationï¼‰â€çš„æ ¸å¿ƒè¯æ®ã€‚

### 3. æ•°æ®é›†åˆ’åˆ† (Data Splits)
è¿™æ˜¯é¢„å¤„ç†è„šæœ¬ `hdfs_preprocess.py` å°†åŸå§‹æ—¥å¿—æ¸…æ´—å¹¶åˆ‡åˆ†åçš„äº§ç‰©ã€‚

#### A. è®­ç»ƒç»„ (ç”¨äºæ•™æ¨¡å‹ä»€ä¹ˆæ˜¯â€œæ­£å¸¸â€)
*   **`train.csv`**
    *   **å†…å®¹**: çº¦ 4855 æ¡æ•°æ®ã€‚**å…¨éƒ½æ˜¯æ­£å¸¸æ ·æœ¬ (Label=0)**ã€‚åŒ…å« `BlockId`, `EventSequence` ç­‰åˆ—ã€‚
    *   **ç”¨é€”**: `train.py` è¯»å–å®ƒè¿›è¡Œè®­ç»ƒã€‚`train_tokenizer.py` è¯»å–å®ƒå»ºç«‹è¯è¡¨ã€‚
*   **`train.txt`**
    *   **å†…å®¹**: `train.csv` çš„çº¯æ–‡æœ¬ç‰ˆæœ¬ï¼ˆå»é™¤äº† BlockId å’Œè¡¨å¤´ï¼‰ã€‚
    *   **ç”¨é€”**: ä¸“é—¨å–‚ç»™ `train_tokenizer.py` çš„ï¼Œå› ä¸º HuggingFace çš„è®­ç»ƒå‡½æ•°åªåƒçº¯æ–‡æœ¬ã€‚

#### B. æµ‹è¯•ç»„ - æ­£å¸¸ (ç”¨äºéªŒè¯è¯¯æŠ¥ç‡ False Positive)
*   **`test_normal.csv`**
    *   **å†…å®¹**: æœªå‚ä¸è®­ç»ƒçš„æ­£å¸¸æ ·æœ¬ã€‚
    *   **ç”¨é€”**:
        1.  åœ¨ `train.py` ä¸­ä½œä¸º **éªŒè¯é›†** (Validation Set) æŒ‡å¯¼ Early Stoppingã€‚
        2.  åœ¨ `predict.py` ä¸­ä½œä¸º **æµ‹è¯•é›†**ï¼Œæ¨¡å‹**ä¸åº”è¯¥**å¯¹å®ƒä»¬æŠ¥è­¦ã€‚
*   **`test_normal.txt`**
    *   **å†…å®¹**: çº¯æ–‡æœ¬ç‰ˆæœ¬ã€‚
    *   **ç”¨é€”**: å…¼å®¹æ€§å¤‡ä»½ï¼Œæ–¹ä¾¿è‚‰çœ¼æŸ¥çœ‹æ¸…æ´—æ•ˆæœã€‚

#### C. æµ‹è¯•ç»„ - å¼‚å¸¸ (ç”¨äºéªŒè¯å¬å›ç‡ Recall)
*   **`test_abnormal.csv`**
    *   **å†…å®¹**: HDFS ä¸­æ‰€æœ‰çš„å¼‚å¸¸æ ·æœ¬ (Label=1)ã€‚
    *   **ç”¨é€”**: **è€ƒå·**ã€‚åœ¨ `predict.py` ä¸­ä½¿ç”¨ã€‚æ¨¡å‹**å¿…é¡»**å¯¹å®ƒä»¬æŠ¥è­¦ã€‚å¦‚æœæ²¡æŠ¥ï¼Œå°±æ˜¯æ¼æŠ¥ï¼ˆFalse Negativeï¼‰ã€‚
*   **`test_abnormal.txt`**
    *   **å†…å®¹**: çº¯æ–‡æœ¬ç‰ˆæœ¬ã€‚
    *   **ç”¨é€”**: å…¼å®¹æ€§å¤‡ä»½ã€‚

#### D. æ€»é›† (Master Copy)
*   **`hdfs_sequence_sanitized.csv`**
    *   **å†…å®¹**: æ¸…æ´—åçš„å…¨é‡æ•°æ®ã€‚
    *   **ç”¨é€”**: **æ•°æ®æ¯ç‰ˆ**ã€‚å¦‚æœä»¥åæƒ³æ”¹å˜ Train/Test çš„åˆ‡åˆ†æ¯”ä¾‹ï¼ˆæ¯”å¦‚ä» 8:2 æ”¹æˆ 5:5ï¼‰ï¼Œä¸éœ€è¦é‡æ–°è·‘æ­£åˆ™æ¸…æ´—ï¼Œç›´æ¥è¯»è¿™ä¸ªæ–‡ä»¶é‡æ–°åˆ‡åˆ†å³å¯ã€‚

---

### æ•°æ®æµå‘å›¾ (Data Flow)

```mermaid
graph TD
    Raw[HDFS.log] --> |Step 1: hdfs_preprocess.py| Master[hdfs_sequence_sanitized.csv]
    
    Master --> |Split| TrainCSV[train.csv]
    Master --> |Split| TestNorm[test_normal.csv]
    Master --> |Split| TestAbnorm[test_abnormal.csv]
    
    TrainCSV --> |Extract Text| TrainTXT[train.txt]
    TrainTXT --> |Step 2: train_tokenizer.py| Vocab[vocab.txt]
    
    TrainCSV & Vocab --> |Step 3: train.py| Model[best_model.pth]
    TestNorm --> |Early Stopping| Model
    
    Model & Vocab & TestNorm & TestAbnorm --> |Step 4: predict.py| Result[F1 Score / Paper Tables]
```

æ‰€æœ‰æ–‡ä»¶éƒ½åœ¨å®ƒä»¬è¯¥åœ¨çš„ä½ç½®ã€‚ç°åœ¨ï¼Œè¯·å¼€å§‹ç¼–å†™æœ€åçš„ **`predict.py`**ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹è¿™å¥—ç³»ç»Ÿçš„æœ€ç»ˆæˆç»©å•ï¼

```angular2html
D:\develop\miniconda3\envs\berttorch\python.exe D:\OtherProjects\BERT-pytorch\hdfs\bt5_train.py 
[-] Random seed set to 42
[-] Training on cuda
[-] Loading data from ../output/hdfs/train.csv...
    Loaded 4855 samples.
[-] Splitting: Train=4370, Val=485
[-] Vocab Size: 424
[-] Initializing Hypersphere Center...
Init Center:   0%|          | 0/68 [00:00<?, ?it/s]D:\develop\miniconda3\envs\berttorch\lib\site-packages\transformers\models\bert\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:555.)
  attn_output = torch.nn.functional.scaled_dot_product_attention(
Init Center: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:23<00:00,  2.90it/s]
Epoch 1/50 [Train]:   0%|          | 0/68 [00:00<?, ?it/s][-] Center initialized. Norm: 2.2375
Epoch 1/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.03it/s, Loss=1.1260, MLM_Acc=49.16%]
Epoch 1: Train Loss=2.5890, Val Loss=1.0782, Train Acc=49.16%
    [*] Saving new best model...
Epoch 2/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.03it/s, Loss=0.7034, MLM_Acc=71.72%]
Epoch 2: Train Loss=0.9647, Val Loss=0.7875, Train Acc=71.72%
    [*] Saving new best model...
Epoch 3/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.05it/s, Loss=0.5328, MLM_Acc=75.20%]
Epoch 3: Train Loss=0.7751, Val Loss=0.6073, Train Acc=75.20%
    [*] Saving new best model...
Epoch 4/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.05it/s, Loss=0.6828, MLM_Acc=81.00%]
Epoch 4: Train Loss=0.6179, Val Loss=0.4753, Train Acc=81.00%
    [*] Saving new best model...
Epoch 5/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.05it/s, Loss=0.6880, MLM_Acc=83.94%]
Epoch 5: Train Loss=0.5276, Val Loss=0.4019, Train Acc=83.94%
    [*] Saving new best model...
Epoch 6/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.05it/s, Loss=0.3838, MLM_Acc=86.20%]
Epoch 6: Train Loss=0.4619, Val Loss=0.3694, Train Acc=86.20%
    [*] Saving new best model...
Epoch 7/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.04it/s, Loss=0.2537, MLM_Acc=87.86%]
Epoch 7: Train Loss=0.4126, Val Loss=0.3380, Train Acc=87.86%
    [*] Saving new best model...
Epoch 8/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.05it/s, Loss=0.1764, MLM_Acc=88.90%]
Epoch 8: Train Loss=0.3797, Val Loss=0.3003, Train Acc=88.90%
    [*] Saving new best model...
Epoch 9/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.04it/s, Loss=0.3113, MLM_Acc=89.69%]
Epoch 9: Train Loss=0.3489, Val Loss=0.2786, Train Acc=89.69%
    [*] Saving new best model...
Epoch 10/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.04it/s, Loss=0.2645, MLM_Acc=90.23%]
Epoch 10: Train Loss=0.3286, Val Loss=0.2453, Train Acc=90.23%
    [*] Saving new best model...
Epoch 11/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.03it/s, Loss=0.2825, MLM_Acc=90.40%]
Epoch 11: Train Loss=0.3230, Val Loss=0.2444, Train Acc=90.40%
    [*] Saving new best model...
Epoch 12/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.03it/s, Loss=0.2315, MLM_Acc=91.13%]
Epoch 12: Train Loss=0.2962, Val Loss=0.2300, Train Acc=91.13%
    [*] Saving new best model...
Epoch 13/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.04it/s, Loss=0.3061, MLM_Acc=91.57%]
Epoch 13: Train Loss=0.2796, Val Loss=0.2181, Train Acc=91.57%
    [*] Saving new best model...
Epoch 14/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.04it/s, Loss=0.1373, MLM_Acc=91.67%]
Epoch 14: Train Loss=0.2757, Val Loss=0.2149, Train Acc=91.67%
    [*] Saving new best model...
Epoch 15/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.03it/s, Loss=0.3141, MLM_Acc=92.09%]
Epoch 15: Train Loss=0.2635, Val Loss=0.2003, Train Acc=92.09%
    [*] Saving new best model...
Epoch 16/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.04it/s, Loss=0.1660, MLM_Acc=92.15%]
Epoch 16: Train Loss=0.2611, Val Loss=0.1901, Train Acc=92.15%
    [*] Saving new best model...
Epoch 17/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.05it/s, Loss=0.1490, MLM_Acc=92.42%]
Epoch 17: Train Loss=0.2498, Val Loss=0.1871, Train Acc=92.42%
    [*] Saving new best model...
Epoch 18/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.05it/s, Loss=0.2191, MLM_Acc=92.57%]
Epoch 19/50 [Train]:   0%|          | 0/68 [00:00<?, ?it/s]Epoch 18: Train Loss=0.2479, Val Loss=0.1931, Train Acc=92.57%
    [!] Patience: 1/3
Epoch 19/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.05it/s, Loss=0.2343, MLM_Acc=92.91%]
Epoch 20/50 [Train]:   0%|          | 0/68 [00:00<?, ?it/s]Epoch 19: Train Loss=0.2366, Val Loss=0.1896, Train Acc=92.91%
    [!] Patience: 2/3
Epoch 20/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.04it/s, Loss=0.1470, MLM_Acc=93.06%]
Epoch 20: Train Loss=0.2284, Val Loss=0.1730, Train Acc=93.06%
    [*] Saving new best model...
Epoch 21/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.05it/s, Loss=0.2447, MLM_Acc=93.31%]
Epoch 21: Train Loss=0.2192, Val Loss=0.1672, Train Acc=93.31%
    [*] Saving new best model...
Epoch 22/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.04it/s, Loss=0.1501, MLM_Acc=93.58%]
Epoch 22: Train Loss=0.2108, Val Loss=0.1639, Train Acc=93.58%
    [*] Saving new best model...
Epoch 23/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.03it/s, Loss=0.2981, MLM_Acc=93.66%]
Epoch 23: Train Loss=0.2049, Val Loss=0.1508, Train Acc=93.66%
    [*] Saving new best model...
Epoch 24/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.02it/s, Loss=0.1737, MLM_Acc=93.93%]
Epoch 24: Train Loss=0.1975, Val Loss=0.1479, Train Acc=93.93%
    [*] Saving new best model...
Epoch 25/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.03it/s, Loss=0.1728, MLM_Acc=94.05%]
Epoch 26/50 [Train]:   0%|          | 0/68 [00:00<?, ?it/s]Epoch 25: Train Loss=0.1912, Val Loss=0.1486, Train Acc=94.05%
    [!] Patience: 1/3
Epoch 26/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.05it/s, Loss=0.1722, MLM_Acc=94.16%]
Epoch 26: Train Loss=0.1873, Val Loss=0.1463, Train Acc=94.16%
    [*] Saving new best model...
Epoch 27/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.04it/s, Loss=0.1962, MLM_Acc=94.48%]
Epoch 27: Train Loss=0.1759, Val Loss=0.1277, Train Acc=94.48%
    [*] Saving new best model...
Epoch 28/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.02it/s, Loss=0.2540, MLM_Acc=94.58%]
Epoch 28: Train Loss=0.1717, Val Loss=0.1310, Train Acc=94.58%
    [!] Patience: 1/3
Epoch 29/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.04it/s, Loss=0.2501, MLM_Acc=94.82%]
Epoch 29: Train Loss=0.1629, Val Loss=0.1180, Train Acc=94.82%
    [*] Saving new best model...
Epoch 30/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.03it/s, Loss=0.1083, MLM_Acc=94.81%]
Epoch 30: Train Loss=0.1620, Val Loss=0.1226, Train Acc=94.81%
    [!] Patience: 1/3
Epoch 31/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.04it/s, Loss=0.1383, MLM_Acc=95.25%]
Epoch 31: Train Loss=0.1470, Val Loss=0.1084, Train Acc=95.25%
    [*] Saving new best model...
Epoch 32/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.03it/s, Loss=0.1259, MLM_Acc=95.34%]
Epoch 32: Train Loss=0.1425, Val Loss=0.0953, Train Acc=95.34%
    [*] Saving new best model...
Epoch 33/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.05it/s, Loss=0.0796, MLM_Acc=95.50%]
Epoch 33: Train Loss=0.1361, Val Loss=0.0916, Train Acc=95.50%
    [*] Saving new best model...
Epoch 34/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.05it/s, Loss=0.1263, MLM_Acc=95.72%]
Epoch 34: Train Loss=0.1268, Val Loss=0.0865, Train Acc=95.72%
    [*] Saving new best model...
Epoch 35/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.04it/s, Loss=0.1399, MLM_Acc=96.00%]
Epoch 35: Train Loss=0.1179, Val Loss=0.0713, Train Acc=96.00%
    [*] Saving new best model...
Epoch 36/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.04it/s, Loss=0.1685, MLM_Acc=96.13%]
Epoch 36: Train Loss=0.1143, Val Loss=0.0690, Train Acc=96.13%
    [*] Saving new best model...
Epoch 37/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.02it/s, Loss=0.1700, MLM_Acc=96.39%]
Epoch 37: Train Loss=0.1057, Val Loss=0.0607, Train Acc=96.39%
    [*] Saving new best model...
Epoch 38/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.04it/s, Loss=0.0659, MLM_Acc=96.74%]
Epoch 38: Train Loss=0.0959, Val Loss=0.0523, Train Acc=96.74%
    [*] Saving new best model...
Epoch 39/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.03it/s, Loss=0.1473, MLM_Acc=96.87%]
Epoch 39: Train Loss=0.0928, Val Loss=0.0453, Train Acc=96.87%
    [*] Saving new best model...
Epoch 40/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.03it/s, Loss=0.0819, MLM_Acc=97.12%]
Epoch 40: Train Loss=0.0842, Val Loss=0.0448, Train Acc=97.12%
    [*] Saving new best model...
Epoch 41/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.04it/s, Loss=0.0373, MLM_Acc=97.36%]
Epoch 41: Train Loss=0.0781, Val Loss=0.0349, Train Acc=97.36%
    [*] Saving new best model...
Epoch 42/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.04it/s, Loss=0.0425, MLM_Acc=97.63%]
Epoch 42: Train Loss=0.0714, Val Loss=0.0316, Train Acc=97.63%
    [*] Saving new best model...
Epoch 43/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.03it/s, Loss=0.0371, MLM_Acc=97.83%]
Epoch 43: Train Loss=0.0654, Val Loss=0.0274, Train Acc=97.83%
    [*] Saving new best model...
Epoch 44/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.04it/s, Loss=0.0740, MLM_Acc=98.06%]
Epoch 44: Train Loss=0.0589, Val Loss=0.0265, Train Acc=98.06%
    [*] Saving new best model...
Epoch 45/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.05it/s, Loss=0.0496, MLM_Acc=98.20%]
Epoch 45: Train Loss=0.0558, Val Loss=0.0213, Train Acc=98.20%
    [*] Saving new best model...
Epoch 46/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.05it/s, Loss=0.0508, MLM_Acc=98.31%]
Epoch 46: Train Loss=0.0512, Val Loss=0.0214, Train Acc=98.31%
    [!] Patience: 1/3
Epoch 47/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.03it/s, Loss=0.0484, MLM_Acc=98.41%]
Epoch 47: Train Loss=0.0484, Val Loss=0.0171, Train Acc=98.41%
    [*] Saving new best model...
Epoch 48/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.05it/s, Loss=0.0251, MLM_Acc=98.62%]
Epoch 48: Train Loss=0.0435, Val Loss=0.0147, Train Acc=98.62%
    [*] Saving new best model...
Epoch 49/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.05it/s, Loss=0.0386, MLM_Acc=98.68%]
Epoch 49: Train Loss=0.0410, Val Loss=0.0133, Train Acc=98.68%
    [*] Saving new best model...
Epoch 50/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.06it/s, Loss=0.0333, MLM_Acc=98.89%]
Epoch 50: Train Loss=0.0355, Val Loss=0.0116, Train Acc=98.89%
    [*] Saving new best model...

Process finished with exit code 0
```
```angular2html
D:\develop\miniconda3\envs\berttorch\python.exe D:\OtherProjects\BERT-pytorch\hdfs\bp6_predict.py 
[-] Loading model with vocab_size=424...
[-] Center loaded. Norm: 2.2375
[-] Loading data from ../output/hdfs/test_normal.csv...
[-] Loading data from ../output/hdfs/test_abnormal.csv...
[-] Extracting features (K=[5, 10, 15, 20])...
Extracting:   0%|          | 0/109 [00:00<?, ?it/s]D:\develop\miniconda3\envs\berttorch\lib\site-packages\transformers\models\bert\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:555.)
  attn_output = torch.nn.functional.scaled_dot_product_attention(
Extracting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 109/109 [04:50<00:00,  2.67s/it]
Extracting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:07<00:00,  1.96s/it]

============================================================
GRID SEARCH: Z-Score(Dist) + Z-Score(TopK)
============================================================
K   | Alpha | Prec    | Rec     | F1      | AUC    
--------------------------------------------------
10  | 0.0   | 0.9522  | 0.8277  | 0.8856  | 0.9137
10  | 0.1   | 0.9574  | 0.8271  | 0.8875  | 0.9126
10  | 0.2   | 0.9509  | 0.8277  | 0.8850  | 0.9125
10  | 0.3   | 0.9300  | 0.8212  | 0.8722  | 0.9124
10  | 0.4   | 0.9410  | 0.7778  | 0.8517  | 0.9122
15  | 0.0   | 0.9373  | 0.8170  | 0.8730  | 0.9140
15  | 0.1   | 0.9521  | 0.8158  | 0.8787  | 0.9127
15  | 0.2   | 0.9523  | 0.8194  | 0.8809  | 0.9126
15  | 0.3   | 0.9564  | 0.8217  | 0.8840  | 0.9126
15  | 0.4   | 0.9299  | 0.8194  | 0.8711  | 0.9124
20  | 0.1   | 0.9034  | 0.8223  | 0.8610  | 0.9126
20  | 0.2   | 0.9467  | 0.8027  | 0.8688  | 0.9126
20  | 0.3   | 0.9416  | 0.8241  | 0.8790  | 0.9125
20  | 0.4   | 0.9466  | 0.8223  | 0.8801  | 0.9125
20  | 0.5   | 0.9290  | 0.8004  | 0.8599  | 0.9120
============================================================
ğŸ† GLOBAL BEST:
   K        : 10
   Alpha    : 0.1 (0=TopK, 1=Dist)
   F1 Score : 0.8875
   AUC Score: 0.9126

Process finished with exit code 0
```
```angular2html
self.test_ratio = 0.5
D:\develop\miniconda3\envs\berttorch\python.exe D:\OtherProjects\BERT-pytorch\HDFS\bp6_predict.py 
[-] Loaded Tokenizer. Real Vocab Size: 424
[-] Loading model from ../output/hdfs/best_model.pth...
[-] Hypersphere Center loaded. Norm: 2.2375
[-] Loading data from ../output/hdfs/test_normal.csv...
[-] Loading data from ../output/hdfs/test_abnormal.csv...
Extracting Features:   0%|          | 0/541 [00:00<?, ?it/s][-] Extracting features (K=10)...
D:\develop\miniconda3\envs\berttorch\lib\site-packages\transformers\models\bert\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:555.)
  attn_output = torch.nn.functional.scaled_dot_product_attention(
Extracting Features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 541/541 [05:34<00:00,  1.62it/s]
Extracting Features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:33<00:00,  1.94s/it]

============================================================
HYBRID SEARCH (Alpha * Dist + (1-Alpha) * TopK)
============================================================
Alpha  | Prec     | Rec      | F1       | AUC     
--------------------------------------------------
0.0    | 0.9419   | 0.8147   | 0.8737   | 0.9087
0.1    | 0.9421   | 0.8192   | 0.8764   | 0.9132
0.2    | 0.9411   | 0.8179   | 0.8752   | 0.9132
0.3    | 0.9163   | 0.8170   | 0.8638   | 0.9131
0.4    | 0.8899   | 0.8067   | 0.8463   | 0.9129
0.5    | 0.9022   | 0.7636   | 0.8271   | 0.9118
0.6    | 0.7098   | 0.6073   | 0.6546   | 0.9058
0.7    | 0.6619   | 0.3101   | 0.4224   | 0.8865
0.8    | 0.7697   | 0.2465   | 0.3734   | 0.8729
0.9    | 0.7474   | 0.2365   | 0.3593   | 0.8532
1.0    | 0.6960   | 0.2172   | 0.3311   | 0.7623
============================================================
ğŸ† BEST RESULT:
   Alpha    : 0.1 (0=TopK, 1=Dist)
   F1 Score : 0.8764
   AUC Score: 0.9132
>> ç»“è®º: Top-K ä»ç„¶ä¸»å¯¼ã€‚

Process finished with exit code 0
```
```angular2html
self.test_ratio = 1
D:\develop\miniconda3\envs\berttorch\python.exe D:\OtherProjects\BERT-pytorch\HDFS\bp6_predict.py 
[-] Loaded Tokenizer. Real Vocab Size: 424
[-] Loading model from ../output/hdfs/best_model.pth...
[-] Hypersphere Center loaded. Norm: 2.2375
[-] Loading data from ../output/hdfs/test_normal.csv...
[-] Loading data from ../output/hdfs/test_abnormal.csv...
[-] Extracting features (K=10)...
Extracting Features:   0%|          | 0/1081 [00:00<?, ?it/s]D:\develop\miniconda3\envs\berttorch\lib\site-packages\transformers\models\bert\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:555.)
  attn_output = torch.nn.functional.scaled_dot_product_attention(
Extracting Features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1081/1081 [10:42<00:00,  1.68it/s]
Extracting Features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:42<00:00,  1.28s/it]

============================================================
HYBRID SEARCH (Alpha * Dist + (1-Alpha) * TopK)
============================================================
Alpha  | Prec     | Rec      | F1       | AUC     
--------------------------------------------------
0.0    | 0.9441   | 0.8179   | 0.8765   | 0.9104
0.1    | 0.9443   | 0.8224   | 0.8792   | 0.9159
0.2    | 0.9440   | 0.8214   | 0.8784   | 0.9159
0.3    | 0.9181   | 0.8207   | 0.8667   | 0.9158
0.4    | 0.8931   | 0.8093   | 0.8491   | 0.9157
0.5    | 0.9050   | 0.7669   | 0.8303   | 0.9146
0.6    | 0.7142   | 0.6127   | 0.6595   | 0.9085
0.7    | 0.6649   | 0.3093   | 0.4222   | 0.8892
0.8    | 0.7652   | 0.2461   | 0.3724   | 0.8753
0.9    | 0.7469   | 0.2345   | 0.3570   | 0.8553
1.0    | 0.6932   | 0.2158   | 0.3291   | 0.7630
============================================================
ğŸ† BEST RESULT:
   Alpha    : 0.1 (0=TopK, 1=Dist)
   F1 Score : 0.8792
   AUC Score: 0.9159
>> ç»“è®º: Top-K ä»ç„¶ä¸»å¯¼ã€‚

Process finished with exit code 0
```
```angular2html
D:\develop\miniconda3\envs\berttorch\python.exe D:\OtherProjects\BERT-pytorch\HDFS\bp6_predict.py 
[-] Loading model with vocab_size=424...
[-] Center loaded. Norm: 2.2375
[-] Loading data from ../output/hdfs/test_normal.csv...
[-] Loading data from ../output/hdfs/test_abnormal.csv...
[-] Extracting features (K=[5, 10, 15, 20])...
Extracting:   0%|          | 0/109 [00:00<?, ?it/s]D:\develop\miniconda3\envs\berttorch\lib\site-packages\transformers\models\bert\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:555.)
  attn_output = torch.nn.functional.scaled_dot_product_attention(
Extracting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 109/109 [04:55<00:00,  2.71s/it]
Extracting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:08<00:00,  2.03s/it]

============================================================
GRID SEARCH: Z-Score(Dist) + Z-Score(TopK)
============================================================
K   | Alpha | Prec    | Rec     | F1      | AUC    
--------------------------------------------------
10  | 0.0   | 0.9469  | 0.8259  | 0.8823  | 0.9139
10  | 0.1   | 0.9553  | 0.8259  | 0.8859  | 0.9278
10  | 0.2   | 0.9496  | 0.8283  | 0.8848  | 0.9277
10  | 0.3   | 0.9302  | 0.8235  | 0.8736  | 0.9276
10  | 0.4   | 0.9261  | 0.7891  | 0.8521  | 0.9275
15  | 0.0   | 0.9272  | 0.8170  | 0.8686  | 0.9144
15  | 0.1   | 0.9548  | 0.8158  | 0.8798  | 0.9279
15  | 0.2   | 0.9559  | 0.8247  | 0.8855  | 0.9278
15  | 0.3   | 0.9554  | 0.8277  | 0.8870  | 0.9277
15  | 0.4   | 0.9407  | 0.8194  | 0.8758  | 0.9276
20  | 0.0   | 0.9476  | 0.7843  | 0.8583  | 0.9109
20  | 0.1   | 0.9428  | 0.8229  | 0.8788  | 0.9279
20  | 0.2   | 0.9542  | 0.8170  | 0.8803  | 0.9278
20  | 0.3   | 0.9513  | 0.8241  | 0.8832  | 0.9278
20  | 0.4   | 0.9500  | 0.8241  | 0.8826  | 0.9276
20  | 0.5   | 0.9146  | 0.7956  | 0.8510  | 0.9273
============================================================
ğŸ† GLOBAL BEST:
   K        : 15
   Alpha    : 0.3 (0=TopK, 1=Dist)
   F1 Score : 0.8870
   AUC Score: 0.9277

Process finished with exit code 0
```