ä¸ºäº†è®©ä½ å¯¹æ•´ä¸ªå®éªŒæµç¨‹çš„æ•°æ®æµå‘ï¼ˆData Pipelineï¼‰ä¸€ç›®äº†ç„¶ï¼Œæˆ‘å°†è¿™äº›æ–‡ä»¶æŒ‰**ç”Ÿæˆé˜¶æ®µ**å’Œ**åŠŸèƒ½ç”¨é€”**è¿›è¡Œäº†åˆ†ç±»æ ‡æ³¨ã€‚

è¿™äº›æ–‡ä»¶æ„æˆäº†ä½ è®ºæ–‡å®éªŒçš„å®Œæ•´è¯æ®é“¾ã€‚

### 1. æ ¸å¿ƒäº§ç‰© (The "Brain")
è¿™æ˜¯è®­ç»ƒè„šæœ¬æœ€ç»ˆç”Ÿæˆçš„ã€ä»·å€¼æœ€é«˜çš„æ–‡ä»¶ã€‚

*   **`best_model.pth`**
    *   **æ¥æº**: `train.py` åœ¨ Early Stopping è¿‡ç¨‹ä¸­ä¿å­˜çš„ã€‚
    *   **å†…å®¹**: åŒ…å«ä¸¤ä¸ªå…³é”®å¯¹è±¡ï¼š
        1.  `model_state_dict`: TinyBERT çš„æ‰€æœ‰æƒé‡å‚æ•°ï¼ˆå·²å­¦ä¼š HDFS è¯­æ³•ï¼‰ã€‚
        2.  `center`: è®­ç»ƒå¥½çš„è¶…çƒä½“ä¸­å¿ƒå‘é‡ï¼ˆæ­£å¸¸æ—¥å¿—çš„â€œèšç±»ä¸­å¿ƒâ€ï¼‰ã€‚
    *   **ç”¨é€”**: **æ¨ç†æ ¸å¿ƒ**ã€‚`predict.py` å°†åŠ è½½å®ƒæ¥åˆ¤æ–­æ–°æ—¥å¿—ç¦»è¿™ä¸ªä¸­å¿ƒæœ‰å¤šè¿œã€‚

### 2. åŸºç¡€è®¾æ–½ (Infrastructure)
è¿™æ˜¯è®© BERT ç†è§£æ—¥å¿—â€œè¯­è¨€â€çš„å­—å…¸ã€‚

*   **`vocab.txt`**
    *   **æ¥æº**: `train_tokenizer.py` (åŸºäº `train.txt` è®­ç»ƒç”Ÿæˆ)ã€‚
    *   **å†…å®¹**: çº¦ 3000 è¡Œã€‚æ¯è¡Œä¸€ä¸ª Tokenï¼ˆå¦‚ `[BLK]`, `packet`, `##ponder`ï¼‰ã€‚
    *   **ç”¨é€”**: **ç¿»è¯‘å®˜**ã€‚å®ƒè´Ÿè´£æŠŠæ–‡æœ¬æ—¥å¿—è½¬æ¢æˆæ•°å­— ID åºåˆ—ã€‚å®ƒæ˜¯ä½ â€œé¢†åŸŸè‡ªé€‚åº”ï¼ˆDomain Adaptationï¼‰â€çš„æ ¸å¿ƒè¯æ®ã€‚

### 3. æ•°æ®é›†åˆ’åˆ† (Data Splits)
è¿™æ˜¯é¢„å¤„ç†è„šæœ¬ `hdfs_preprocess.py` å°†åŸå§‹æ—¥å¿—æ¸…æ´—å¹¶åˆ‡åˆ†åçš„äº§ç‰©ã€‚

#### A. è®­ç»ƒç»„ (ç”¨äºæ•™æ¨¡å‹ä»€ä¹ˆæ˜¯â€œæ­£å¸¸â€)
*   **`train.csv`**
    *   **å†…å®¹**: çº¦ 4855 æ¡æ•°æ®ã€‚**å…¨éƒ½æ˜¯æ­£å¸¸æ ·æœ¬ (Label=0)**ã€‚åŒ…å« `BlockId`, `EventSequence` ç­‰åˆ—ã€‚
    *   **ç”¨é€”**: `train.py` è¯»å–å®ƒè¿›è¡Œè®­ç»ƒã€‚`train_tokenizer.py` è¯»å–å®ƒå»ºç«‹è¯è¡¨ã€‚
*   **`train.txt`**
    *   **å†…å®¹**: `train.csv` çš„çº¯æ–‡æœ¬ç‰ˆæœ¬ï¼ˆå»é™¤äº† BlockId å’Œè¡¨å¤´ï¼‰ã€‚
    *   **ç”¨é€”**: ä¸“é—¨å–‚ç»™ `train_tokenizer.py` çš„ï¼Œå› ä¸º HuggingFace çš„è®­ç»ƒå‡½æ•°åªåƒçº¯æ–‡æœ¬ã€‚

#### B. æµ‹è¯•ç»„ - æ­£å¸¸ (ç”¨äºéªŒè¯è¯¯æŠ¥ç‡ False Positive)
*   **`test_normal.csv`**
    *   **å†…å®¹**: æœªå‚ä¸è®­ç»ƒçš„æ­£å¸¸æ ·æœ¬ã€‚
    *   **ç”¨é€”**:
        1.  åœ¨ `train.py` ä¸­ä½œä¸º **éªŒè¯é›†** (Validation Set) æŒ‡å¯¼ Early Stoppingã€‚
        2.  åœ¨ `predict.py` ä¸­ä½œä¸º **æµ‹è¯•é›†**ï¼Œæ¨¡å‹**ä¸åº”è¯¥**å¯¹å®ƒä»¬æŠ¥è­¦ã€‚
*   **`test_normal.txt`**
    *   **å†…å®¹**: çº¯æ–‡æœ¬ç‰ˆæœ¬ã€‚
    *   **ç”¨é€”**: å…¼å®¹æ€§å¤‡ä»½ï¼Œæ–¹ä¾¿è‚‰çœ¼æŸ¥çœ‹æ¸…æ´—æ•ˆæœã€‚

#### C. æµ‹è¯•ç»„ - å¼‚å¸¸ (ç”¨äºéªŒè¯å¬å›ç‡ Recall)
*   **`test_abnormal.csv`**
    *   **å†…å®¹**: HDFS ä¸­æ‰€æœ‰çš„å¼‚å¸¸æ ·æœ¬ (Label=1)ã€‚
    *   **ç”¨é€”**: **è€ƒå·**ã€‚åœ¨ `predict.py` ä¸­ä½¿ç”¨ã€‚æ¨¡å‹**å¿…é¡»**å¯¹å®ƒä»¬æŠ¥è­¦ã€‚å¦‚æœæ²¡æŠ¥ï¼Œå°±æ˜¯æ¼æŠ¥ï¼ˆFalse Negativeï¼‰ã€‚
*   **`test_abnormal.txt`**
    *   **å†…å®¹**: çº¯æ–‡æœ¬ç‰ˆæœ¬ã€‚
    *   **ç”¨é€”**: å…¼å®¹æ€§å¤‡ä»½ã€‚

#### D. æ€»é›† (Master Copy)
*   **`hdfs_sequence_sanitized.csv`**
    *   **å†…å®¹**: æ¸…æ´—åçš„å…¨é‡æ•°æ®ã€‚
    *   **ç”¨é€”**: **æ•°æ®æ¯ç‰ˆ**ã€‚å¦‚æœä»¥åæƒ³æ”¹å˜ Train/Test çš„åˆ‡åˆ†æ¯”ä¾‹ï¼ˆæ¯”å¦‚ä» 8:2 æ”¹æˆ 5:5ï¼‰ï¼Œä¸éœ€è¦é‡æ–°è·‘æ­£åˆ™æ¸…æ´—ï¼Œç›´æ¥è¯»è¿™ä¸ªæ–‡ä»¶é‡æ–°åˆ‡åˆ†å³å¯ã€‚

---

### æ•°æ®æµå‘å›¾ (Data Flow)

```mermaid
graph TD
    Raw[HDFS.log] --> |Step 1: hdfs_preprocess.py| Master[hdfs_sequence_sanitized.csv]
    
    Master --> |Split| TrainCSV[train.csv]
    Master --> |Split| TestNorm[test_normal.csv]
    Master --> |Split| TestAbnorm[test_abnormal.csv]
    
    TrainCSV --> |Extract Text| TrainTXT[train.txt]
    TrainTXT --> |Step 2: train_tokenizer.py| Vocab[vocab.txt]
    
    TrainCSV & Vocab --> |Step 3: train.py| Model[best_model.pth]
    TestNorm --> |Early Stopping| Model
    
    Model & Vocab & TestNorm & TestAbnorm --> |Step 4: predict.py| Result[F1 Score / Paper Tables]
```

æ‰€æœ‰æ–‡ä»¶éƒ½åœ¨å®ƒä»¬è¯¥åœ¨çš„ä½ç½®ã€‚ç°åœ¨ï¼Œè¯·å¼€å§‹ç¼–å†™æœ€åçš„ **`predict.py`**ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹è¿™å¥—ç³»ç»Ÿçš„æœ€ç»ˆæˆç»©å•ï¼

```angular2html
D:\develop\miniconda3\envs\berttorch\python.exe D:\OtherProjects\BERT-pytorch\HDFS\bt5_train.py 
[-] Random seed set to 42
[-] Training on cuda
[-] Loading data from ../output/hdfs/train.csv...
    Loaded 4855 samples.
[-] Splitting: Train=4370, Val=485
[-] Vocab Size: 424
[-] Initializing Hypersphere Center...
Init Center:   0%|          | 0/68 [00:00<?, ?it/s]D:\develop\miniconda3\envs\berttorch\lib\site-packages\transformers\models\bert\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:555.)
  attn_output = torch.nn.functional.scaled_dot_product_attention(
Init Center: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:24<00:00,  2.79it/s]
Epoch 1/50 [Train]:   0%|          | 0/68 [00:00<?, ?it/s][-] Center initialized. Norm: 2.2375
Epoch 1/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:35<00:00,  1.94it/s, Loss=1.1260, MLM_Acc=49.16%]
Epoch 1: Train Loss=2.5890, Val Loss=1.0782, Train Acc=49.16%
    [*] Saving new best model...
Epoch 2/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:35<00:00,  1.93it/s, Loss=0.7034, MLM_Acc=71.72%]
Epoch 2: Train Loss=0.9647, Val Loss=0.7875, Train Acc=71.72%
    [*] Saving new best model...
Epoch 3/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:34<00:00,  1.99it/s, Loss=0.5328, MLM_Acc=75.20%]
Epoch 3: Train Loss=0.7751, Val Loss=0.6073, Train Acc=75.20%
    [*] Saving new best model...
Epoch 4/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.01it/s, Loss=0.6828, MLM_Acc=81.00%]
Epoch 4: Train Loss=0.6179, Val Loss=0.4753, Train Acc=81.00%
    [*] Saving new best model...
Epoch 5/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:34<00:00,  1.96it/s, Loss=0.6880, MLM_Acc=83.94%]
Epoch 5: Train Loss=0.5276, Val Loss=0.4019, Train Acc=83.94%
    [*] Saving new best model...
Epoch 6/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:34<00:00,  1.95it/s, Loss=0.3838, MLM_Acc=86.20%]
Epoch 6: Train Loss=0.4619, Val Loss=0.3694, Train Acc=86.20%
    [*] Saving new best model...
Epoch 7/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:34<00:00,  1.96it/s, Loss=0.2537, MLM_Acc=87.86%]
Epoch 7: Train Loss=0.4126, Val Loss=0.3380, Train Acc=87.86%
    [*] Saving new best model...
Epoch 8/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:34<00:00,  1.95it/s, Loss=0.1764, MLM_Acc=88.90%]
Epoch 8: Train Loss=0.3797, Val Loss=0.3003, Train Acc=88.90%
    [*] Saving new best model...
Epoch 9/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:34<00:00,  1.99it/s, Loss=0.3113, MLM_Acc=89.69%]
Epoch 9: Train Loss=0.3489, Val Loss=0.2786, Train Acc=89.69%
    [*] Saving new best model...
Epoch 10/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:35<00:00,  1.94it/s, Loss=0.2645, MLM_Acc=90.23%]
Epoch 10: Train Loss=0.3286, Val Loss=0.2453, Train Acc=90.23%
    [*] Saving new best model...
Epoch 11/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:34<00:00,  1.98it/s, Loss=0.2825, MLM_Acc=90.40%]
Epoch 11: Train Loss=0.3230, Val Loss=0.2444, Train Acc=90.40%
    [*] Saving new best model...
Epoch 12/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:34<00:00,  1.98it/s, Loss=0.2315, MLM_Acc=91.13%]
Epoch 12: Train Loss=0.2962, Val Loss=0.2300, Train Acc=91.13%
    [*] Saving new best model...
Epoch 13/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:34<00:00,  1.97it/s, Loss=0.3062, MLM_Acc=91.57%]
Epoch 13: Train Loss=0.2796, Val Loss=0.2181, Train Acc=91.57%
    [*] Saving new best model...
Epoch 14/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:34<00:00,  1.96it/s, Loss=0.1373, MLM_Acc=91.67%]
Epoch 14: Train Loss=0.2757, Val Loss=0.2149, Train Acc=91.67%
    [*] Saving new best model...
Epoch 15/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:34<00:00,  1.99it/s, Loss=0.3140, MLM_Acc=92.09%]
Epoch 15: Train Loss=0.2635, Val Loss=0.2002, Train Acc=92.09%
    [*] Saving new best model...
Epoch 16/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:34<00:00,  1.95it/s, Loss=0.1659, MLM_Acc=92.15%]
Epoch 16: Train Loss=0.2611, Val Loss=0.1901, Train Acc=92.15%
    [*] Saving new best model...
Epoch 17/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:34<00:00,  1.95it/s, Loss=0.1486, MLM_Acc=92.42%]
Epoch 17: Train Loss=0.2499, Val Loss=0.1871, Train Acc=92.42%
    [*] Saving new best model...
Epoch 18/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:34<00:00,  1.95it/s, Loss=0.2196, MLM_Acc=92.57%]
Epoch 18: Train Loss=0.2479, Val Loss=0.1932, Train Acc=92.57%
    [!] Patience: 1/3
Epoch 19/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:36<00:00,  1.87it/s, Loss=0.2342, MLM_Acc=92.90%]
Epoch 20/50 [Train]:   0%|          | 0/68 [00:00<?, ?it/s]Epoch 19: Train Loss=0.2366, Val Loss=0.1897, Train Acc=92.90%
    [!] Patience: 2/3
Epoch 20/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:34<00:00,  1.98it/s, Loss=0.1470, MLM_Acc=93.06%]
Epoch 20: Train Loss=0.2285, Val Loss=0.1731, Train Acc=93.06%
    [*] Saving new best model...
Epoch 21/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:34<00:00,  1.98it/s, Loss=0.2444, MLM_Acc=93.32%]
Epoch 21: Train Loss=0.2192, Val Loss=0.1674, Train Acc=93.32%
    [*] Saving new best model...
Epoch 22/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.01it/s, Loss=0.1496, MLM_Acc=93.59%]
Epoch 22: Train Loss=0.2108, Val Loss=0.1635, Train Acc=93.59%
    [*] Saving new best model...
Epoch 23/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:35<00:00,  1.92it/s, Loss=0.2987, MLM_Acc=93.67%]
Epoch 23: Train Loss=0.2049, Val Loss=0.1507, Train Acc=93.67%
    [*] Saving new best model...
Epoch 24/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:34<00:00,  1.96it/s, Loss=0.1736, MLM_Acc=93.94%]
Epoch 24: Train Loss=0.1975, Val Loss=0.1480, Train Acc=93.94%
    [*] Saving new best model...
Epoch 25/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.01it/s, Loss=0.1723, MLM_Acc=94.04%]
Epoch 26/50 [Train]:   0%|          | 0/68 [00:00<?, ?it/s]Epoch 25: Train Loss=0.1913, Val Loss=0.1485, Train Acc=94.04%
    [!] Patience: 1/3
Epoch 26/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:34<00:00,  1.95it/s, Loss=0.1722, MLM_Acc=94.17%]
Epoch 26: Train Loss=0.1872, Val Loss=0.1459, Train Acc=94.17%
    [*] Saving new best model...
Epoch 27/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:34<00:00,  1.96it/s, Loss=0.1960, MLM_Acc=94.49%]
Epoch 27: Train Loss=0.1757, Val Loss=0.1277, Train Acc=94.49%
    [*] Saving new best model...
Epoch 28/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:34<00:00,  1.95it/s, Loss=0.2537, MLM_Acc=94.58%]
Epoch 29/50 [Train]:   0%|          | 0/68 [00:00<?, ?it/s]Epoch 28: Train Loss=0.1720, Val Loss=0.1312, Train Acc=94.58%
    [!] Patience: 1/3
Epoch 29/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.04it/s, Loss=0.2495, MLM_Acc=94.81%]
Epoch 29: Train Loss=0.1629, Val Loss=0.1175, Train Acc=94.81%
    [*] Saving new best model...
Epoch 30/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.04it/s, Loss=0.1082, MLM_Acc=94.80%]
Epoch 30: Train Loss=0.1615, Val Loss=0.1226, Train Acc=94.80%
    [!] Patience: 1/3
Epoch 31/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.04it/s, Loss=0.1376, MLM_Acc=95.25%]
Epoch 31: Train Loss=0.1468, Val Loss=0.1082, Train Acc=95.25%
    [*] Saving new best model...
Epoch 32/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.04it/s, Loss=0.1265, MLM_Acc=95.33%]
Epoch 32: Train Loss=0.1425, Val Loss=0.0954, Train Acc=95.33%
    [*] Saving new best model...
Epoch 33/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.04it/s, Loss=0.0808, MLM_Acc=95.51%]
Epoch 33: Train Loss=0.1354, Val Loss=0.0906, Train Acc=95.51%
    [*] Saving new best model...
Epoch 34/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.04it/s, Loss=0.1249, MLM_Acc=95.72%]
Epoch 34: Train Loss=0.1272, Val Loss=0.0861, Train Acc=95.72%
    [*] Saving new best model...
Epoch 35/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.01it/s, Loss=0.1402, MLM_Acc=96.01%]
Epoch 35: Train Loss=0.1177, Val Loss=0.0716, Train Acc=96.01%
    [*] Saving new best model...
Epoch 36/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.01it/s, Loss=0.1668, MLM_Acc=96.13%]
Epoch 36: Train Loss=0.1143, Val Loss=0.0690, Train Acc=96.13%
    [*] Saving new best model...
Epoch 37/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.02it/s, Loss=0.1669, MLM_Acc=96.40%]
Epoch 37: Train Loss=0.1058, Val Loss=0.0596, Train Acc=96.40%
    [*] Saving new best model...
Epoch 38/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.02it/s, Loss=0.0651, MLM_Acc=96.74%]
Epoch 38: Train Loss=0.0955, Val Loss=0.0515, Train Acc=96.74%
    [*] Saving new best model...
Epoch 39/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.01it/s, Loss=0.1471, MLM_Acc=96.87%]
Epoch 39: Train Loss=0.0926, Val Loss=0.0454, Train Acc=96.87%
    [*] Saving new best model...
Epoch 40/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.02it/s, Loss=0.0818, MLM_Acc=97.13%]
Epoch 40: Train Loss=0.0843, Val Loss=0.0448, Train Acc=97.13%
    [*] Saving new best model...
Epoch 41/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:34<00:00,  1.99it/s, Loss=0.0388, MLM_Acc=97.37%]
Epoch 41: Train Loss=0.0778, Val Loss=0.0364, Train Acc=97.37%
    [*] Saving new best model...
Epoch 42/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:34<00:00,  2.00it/s, Loss=0.0428, MLM_Acc=97.64%]
Epoch 42: Train Loss=0.0713, Val Loss=0.0316, Train Acc=97.64%
    [*] Saving new best model...
Epoch 43/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.04it/s, Loss=0.0351, MLM_Acc=97.85%]
Epoch 43: Train Loss=0.0654, Val Loss=0.0267, Train Acc=97.85%
    [*] Saving new best model...
Epoch 44/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.04it/s, Loss=0.0741, MLM_Acc=98.08%]
Epoch 44: Train Loss=0.0585, Val Loss=0.0265, Train Acc=98.08%
    [*] Saving new best model...
Epoch 45/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.04it/s, Loss=0.0512, MLM_Acc=98.15%]
Epoch 45: Train Loss=0.0559, Val Loss=0.0219, Train Acc=98.15%
    [*] Saving new best model...
Epoch 46/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.04it/s, Loss=0.0486, MLM_Acc=98.33%]
Epoch 46: Train Loss=0.0513, Val Loss=0.0213, Train Acc=98.33%
    [*] Saving new best model...
Epoch 47/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.04it/s, Loss=0.0462, MLM_Acc=98.44%]
Epoch 47: Train Loss=0.0481, Val Loss=0.0171, Train Acc=98.44%
    [*] Saving new best model...
Epoch 48/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.04it/s, Loss=0.0258, MLM_Acc=98.62%]
Epoch 48: Train Loss=0.0432, Val Loss=0.0141, Train Acc=98.62%
    [*] Saving new best model...
Epoch 49/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.04it/s, Loss=0.0386, MLM_Acc=98.67%]
Epoch 49: Train Loss=0.0406, Val Loss=0.0134, Train Acc=98.67%
    [*] Saving new best model...
Epoch 50/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.04it/s, Loss=0.0388, MLM_Acc=98.88%]
Epoch 50: Train Loss=0.0356, Val Loss=0.0116, Train Acc=98.88%
    [*] Saving new best model...
```
```angular2html
self.test_ratio = 0.1
D:\develop\miniconda3\envs\berttorch\python.exe D:\OtherProjects\BERT-pytorch\HDFS\bp6_predict.py
[-] Loaded Tokenizer. Real Vocab Size: 424
[-] Loading model from ../output/hdfs/best_model.pth...
[-] Hypersphere Center loaded. Norm: 2.2375
[-] Loading data from ../output/hdfs/test_normal.csv...
[-] Loading data from ../output/hdfs/test_abnormal.csv...
Extracting Features:   0%|          | 0/109 [00:00<?, ?it/s][-] Extracting features (K=10)...
D:\develop\miniconda3\envs\berttorch\lib\site-packages\transformers\models\bert\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\actions-runner_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:555.)
attn_output = torch.nn.functional.scaled_dot_product_attention(
Extracting Features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 109/109 [01:30<00:00,  1.21it/s]
Extracting Features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:25<00:00,  6.47s/it]
============================================================
HYBRID SEARCH (Alpha * Dist + (1-Alpha) * TopK)
Alpha  | Prec     | Rec      | F1       | AUC
0.0    | 0.9533   | 0.8241   | 0.8840   | 0.9139
0.1    | 0.9535   | 0.8289   | 0.8868   | 0.9179
0.2    | 0.9541   | 0.8271   | 0.8861   | 0.9178
0.3    | 0.9364   | 0.8217   | 0.8753   | 0.9177
0.4    | 0.8955   | 0.8200   | 0.8561   | 0.9175
0.5    | 0.9232   | 0.7576   | 0.8322   | 0.9163
0.6    | 0.6656   | 0.6090   | 0.6361   | 0.9089
0.7    | 0.7990   | 0.2763   | 0.4106   | 0.8895
0.8    | 0.8012   | 0.2442   | 0.3743   | 0.8760
0.9    | 0.7952   | 0.2377   | 0.3660   | 0.8566
1.0    | 0.7162   | 0.2175   | 0.3336   | 0.7658
ğŸ† BEST RESULT:
Alpha    : 0.1 (0=TopK, 1=Dist)
F1 Score : 0.8868
AUC Score: 0.9179
ç»“è®º: æ··åˆç­–ç•¥æœ‰æ•ˆï¼
Process finished with exit code 0
```
```angular2html
self.test_ratio = 0.5
D:\develop\miniconda3\envs\berttorch\python.exe D:\OtherProjects\BERT-pytorch\HDFS\bp6_predict.py 
[-] Loaded Tokenizer. Real Vocab Size: 424
[-] Loading model from ../output/hdfs/best_model.pth...
[-] Hypersphere Center loaded. Norm: 2.2375
[-] Loading data from ../output/hdfs/test_normal.csv...
[-] Loading data from ../output/hdfs/test_abnormal.csv...
Extracting Features:   0%|          | 0/541 [00:00<?, ?it/s][-] Extracting features (K=10)...
D:\develop\miniconda3\envs\berttorch\lib\site-packages\transformers\models\bert\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:555.)
  attn_output = torch.nn.functional.scaled_dot_product_attention(
Extracting Features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 541/541 [05:34<00:00,  1.62it/s]
Extracting Features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:33<00:00,  1.94s/it]

============================================================
HYBRID SEARCH (Alpha * Dist + (1-Alpha) * TopK)
============================================================
Alpha  | Prec     | Rec      | F1       | AUC     
--------------------------------------------------
0.0    | 0.9419   | 0.8147   | 0.8737   | 0.9087
0.1    | 0.9421   | 0.8192   | 0.8764   | 0.9132
0.2    | 0.9411   | 0.8179   | 0.8752   | 0.9132
0.3    | 0.9163   | 0.8170   | 0.8638   | 0.9131
0.4    | 0.8899   | 0.8067   | 0.8463   | 0.9129
0.5    | 0.9022   | 0.7636   | 0.8271   | 0.9118
0.6    | 0.7098   | 0.6073   | 0.6546   | 0.9058
0.7    | 0.6619   | 0.3101   | 0.4224   | 0.8865
0.8    | 0.7697   | 0.2465   | 0.3734   | 0.8729
0.9    | 0.7474   | 0.2365   | 0.3593   | 0.8532
1.0    | 0.6960   | 0.2172   | 0.3311   | 0.7623
============================================================
ğŸ† BEST RESULT:
   Alpha    : 0.1 (0=TopK, 1=Dist)
   F1 Score : 0.8764
   AUC Score: 0.9132
>> ç»“è®º: Top-K ä»ç„¶ä¸»å¯¼ã€‚

Process finished with exit code 0
```
```angular2html
self.test_ratio = 1
D:\develop\miniconda3\envs\berttorch\python.exe D:\OtherProjects\BERT-pytorch\HDFS\bp6_predict.py 
[-] Loaded Tokenizer. Real Vocab Size: 424
[-] Loading model from ../output/hdfs/best_model.pth...
[-] Hypersphere Center loaded. Norm: 2.2375
[-] Loading data from ../output/hdfs/test_normal.csv...
[-] Loading data from ../output/hdfs/test_abnormal.csv...
[-] Extracting features (K=10)...
Extracting Features:   0%|          | 0/1081 [00:00<?, ?it/s]D:\develop\miniconda3\envs\berttorch\lib\site-packages\transformers\models\bert\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:555.)
  attn_output = torch.nn.functional.scaled_dot_product_attention(
Extracting Features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1081/1081 [10:42<00:00,  1.68it/s]
Extracting Features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:42<00:00,  1.28s/it]

============================================================
HYBRID SEARCH (Alpha * Dist + (1-Alpha) * TopK)
============================================================
Alpha  | Prec     | Rec      | F1       | AUC     
--------------------------------------------------
0.0    | 0.9441   | 0.8179   | 0.8765   | 0.9104
0.1    | 0.9443   | 0.8224   | 0.8792   | 0.9159
0.2    | 0.9440   | 0.8214   | 0.8784   | 0.9159
0.3    | 0.9181   | 0.8207   | 0.8667   | 0.9158
0.4    | 0.8931   | 0.8093   | 0.8491   | 0.9157
0.5    | 0.9050   | 0.7669   | 0.8303   | 0.9146
0.6    | 0.7142   | 0.6127   | 0.6595   | 0.9085
0.7    | 0.6649   | 0.3093   | 0.4222   | 0.8892
0.8    | 0.7652   | 0.2461   | 0.3724   | 0.8753
0.9    | 0.7469   | 0.2345   | 0.3570   | 0.8553
1.0    | 0.6932   | 0.2158   | 0.3291   | 0.7630
============================================================
ğŸ† BEST RESULT:
   Alpha    : 0.1 (0=TopK, 1=Dist)
   F1 Score : 0.8792
   AUC Score: 0.9159
>> ç»“è®º: Top-K ä»ç„¶ä¸»å¯¼ã€‚

Process finished with exit code 0
```
```angular2html
D:\develop\miniconda3\envs\berttorch\python.exe D:\OtherProjects\BERT-pytorch\HDFS\bp6_predict.py 
[-] Loading model with vocab_size=424...
[-] Center loaded. Norm: 2.2375
[-] Loading data from ../output/hdfs/test_normal.csv...
[-] Loading data from ../output/hdfs/test_abnormal.csv...
[-] Extracting features (K=[5, 10, 15, 20])...
Extracting:   0%|          | 0/109 [00:00<?, ?it/s]D:\develop\miniconda3\envs\berttorch\lib\site-packages\transformers\models\bert\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:555.)
  attn_output = torch.nn.functional.scaled_dot_product_attention(
Extracting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 109/109 [04:55<00:00,  2.71s/it]
Extracting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:08<00:00,  2.03s/it]

============================================================
GRID SEARCH: Z-Score(Dist) + Z-Score(TopK)
============================================================
K   | Alpha | Prec    | Rec     | F1      | AUC    
--------------------------------------------------
10  | 0.0   | 0.9469  | 0.8259  | 0.8823  | 0.9139
10  | 0.1   | 0.9553  | 0.8259  | 0.8859  | 0.9278
10  | 0.2   | 0.9496  | 0.8283  | 0.8848  | 0.9277
10  | 0.3   | 0.9302  | 0.8235  | 0.8736  | 0.9276
10  | 0.4   | 0.9261  | 0.7891  | 0.8521  | 0.9275
15  | 0.0   | 0.9272  | 0.8170  | 0.8686  | 0.9144
15  | 0.1   | 0.9548  | 0.8158  | 0.8798  | 0.9279
15  | 0.2   | 0.9559  | 0.8247  | 0.8855  | 0.9278
15  | 0.3   | 0.9554  | 0.8277  | 0.8870  | 0.9277
15  | 0.4   | 0.9407  | 0.8194  | 0.8758  | 0.9276
20  | 0.0   | 0.9476  | 0.7843  | 0.8583  | 0.9109
20  | 0.1   | 0.9428  | 0.8229  | 0.8788  | 0.9279
20  | 0.2   | 0.9542  | 0.8170  | 0.8803  | 0.9278
20  | 0.3   | 0.9513  | 0.8241  | 0.8832  | 0.9278
20  | 0.4   | 0.9500  | 0.8241  | 0.8826  | 0.9276
20  | 0.5   | 0.9146  | 0.7956  | 0.8510  | 0.9273
============================================================
ğŸ† GLOBAL BEST:
   K        : 15
   Alpha    : 0.3 (0=TopK, 1=Dist)
   F1 Score : 0.8870
   AUC Score: 0.9277

Process finished with exit code 0
```